{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "096ad360-2d82-4d0d-9e55-a701bce8ce2a",
   "metadata": {},
   "source": [
    "# Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87e7ba1-4bc3-47f8-a1f1-e2040044b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796a001-28a7-4f3d-80f7-6117fda0fa9c",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c88022f-e4ff-40c6-9500-df68f6da5356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ropar_i9941t3\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Everything imported succesfully☑️\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "import math\n",
    "print(\"Everything imported succesfully☑️\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a78d304-0946-452b-9be6-b984412f7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Path to your video file\n",
    "video_path = \"Test.mp4\"\n",
    "\n",
    "# Create a VideoCapture object to read from the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Recolor frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "\n",
    "        # Make pose detections\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Recolor back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Extract landmarks and render them\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            # Render detections\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "            )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b933228-e50a-483d-a937-081dce95c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory for the processed video\n",
    "output_dir = \"../output\"  \n",
    "\n",
    "video_path = \"../Test.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14baa30c-0295-4aa5-828d-b57137d4344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define video writer (outside the loop for efficiency)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' for MP4 format\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Get original frame rate\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "out_video = cv2.VideoWriter(f\"{output_dir}/processed_video.mp4\", fourcc, fps, (frame_width, frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3e5c68f-bfcb-4e42-9281-71d858d3d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_pose.Pose(min_detection_confidence=0.8, min_tracking_confidence=0.8) as pose:\n",
    "    frame_count = 0\n",
    "    skip_count = 2  # Number of frames to skip between processing\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Process only every skip_count frame\n",
    "        if frame_count % skip_count == 0:\n",
    "            # Recolor image to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "\n",
    "            # Make detections\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Recolor back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Extract landmarks and render them (optional)\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    results.pose_landmarks,\n",
    "                    mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2),\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Write the processed frame to the output video\n",
    "            out_video.write(image)\n",
    "\n",
    "        # Display the processed frame (if applicable)\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53db5d02-2096-4680-b2ba-23ae21a5aa8d",
   "metadata": {},
   "source": [
    "### Landmarks To Consider\n",
    "- 11 : Left Shoulder\n",
    "- 12 : Right Shoulder\n",
    "- 13 : Left elbow\n",
    "- 14 : Right elbow\n",
    "- 15 : Left wrist\n",
    "- 16:  Right wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcfea3-f805-4eae-97c7-7297fe3f4fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002ac6e-8838-4616-a827-9fa8b98f5a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_cord = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n",
    "rs_cord = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n",
    "\n",
    "le_cord = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n",
    "re_cord = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
    "\n",
    "lw_cord = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n",
    "rw_cord = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ba5d2-b77b-4a53-a040-0e88200b12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_between_points(p1, p2):\n",
    "    \"\"\"\n",
    "    Calculate the angle between two points in 3D space.\n",
    "    \n",
    "    Args:\n",
    "        p1 (Point): First point, with attributes x, y, and z representing its coordinates.\n",
    "        p2 (Point): Second point, with attributes x, y, and z representing its coordinates.\n",
    "        \n",
    "    Returns:\n",
    "        float: Angle between the two points in degrees.\n",
    "        \n",
    "    Notes:\n",
    "        - Requires the 'math' module.\n",
    "        - The function uses the dot product of the vectors formed by the two points\n",
    "          to calculate the angle between them.\n",
    "    \"\"\"\n",
    "    x1, y1, z1 = p1.x, p1.y, p1.z\n",
    "    x2, y2, z2 = p2.x, p2.y, p2.z\n",
    "    \n",
    "    # Calculate the dot product of the two vectors\n",
    "    dot_product = x1 * x2 + y1 * y2 + z1 * z2\n",
    "    \n",
    "    # Calculate the magnitude of each vector\n",
    "    magnitude1 = math.sqrt(x1 ** 2 + y1 ** 2 + z1 ** 2)\n",
    "    magnitude2 = math.sqrt(x2 ** 2 + y2 ** 2 + z2 ** 2)\n",
    "    \n",
    "    # Calculate the cosine of the angle between the vectors\n",
    "    cosine_angle = dot_product / (magnitude1 * magnitude2)\n",
    "    \n",
    "    # Use arccos to get the angle in radians\n",
    "    angle_rad = math.acos(cosine_angle)\n",
    "    \n",
    "    # Convert radians to degrees\n",
    "    angle_deg = math.degrees(angle_rad)\n",
    "    \n",
    "    return angle_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39096d4-5c74-4b4e-94a5-2042d9459a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = angle_between_points(ls_cord,rs_cord)\n",
    "print(\"Angle between the two points:\", angle, \"degrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43abb73-3a01-41d4-9f82-7b52414bf906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665354ff-ffe8-43fe-8cae-42e206759943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Tutor",
   "language": "python",
   "name": "ai_tutor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
