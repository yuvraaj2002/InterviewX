{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f1bea-b584-4280-bc2d-657d269e9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "!pip install langchain\n",
    "!pip install ctransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07299c4-2eb9-434b-babe-110222b1581d",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb651c39-3277-4144-808e-16809a3a2811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported succesfully✅\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import CTransformers\n",
    "import ipywidgets\n",
    "\n",
    "print(\"Everything imported succesfully✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304845b-5d37-4af5-a865-9e1c78b50c85",
   "metadata": {},
   "source": [
    "### Extracting the data from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b9e6284-0658-4e79-a081-216b09606064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "\n",
    "    # Instantiating a pdf reader object\n",
    "    pdf_reader = PdfReader(path)\n",
    "\n",
    "    # Counting the number of pages in the PDF\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Extracting text from the first page of the PDF\n",
    "    if num_pages == 1:\n",
    "        content = pdf_reader.pages[0]\n",
    "        text = content.extract_text()\n",
    "        return text\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e554c6-7d50-4437-9a12-64a29322fffc",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7acb653-0534-44a0-a5ea-d94b070ce857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    llm = CTransformers(\n",
    "        model=\"../Artifacts/llama-2-7b.ggmlv3.q4_1.bin\",\n",
    "        model_type=\"llama\",\n",
    "        config={\"temperature\": 0.5},\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "587804b9-82dd-48bb-a6d2-cd73f6ca2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_responses(Input_text):\n",
    "\n",
    "    # Getting the model\n",
    "    llm_model = load_model()\n",
    "\n",
    "    # Defining the prompt template\n",
    "    Que_template = PromptTemplate(\n",
    "        input_variables=[\"data\"],\n",
    "        template=\"\"\"\n",
    "        Given a candidate's profile stored in the provided {data} text, automatically extract the name of the candidate.\n",
    "        \"\"\",\n",
    "    )\n",
    "    model_response = llm_model(Que_template.format(data=Input_text))\n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc5a1001-ca1d-4a89-af5d-4ffeea318f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_response(Input_text):\n",
    "\n",
    "    # Getting the model\n",
    "    llm_model = load_model()\n",
    "\n",
    "    # Defining the prompt template\n",
    "    Que_template = PromptTemplate(\n",
    "        input_variables=[\"data\"],\n",
    "        template=\"\"\"\n",
    "        Given a candidate's profile stored in the provided {data} text, automatically extract information about their projects, experience, and achievements.\n",
    "        Utilize this extracted information to craft 10 interview questions that an interviewer might ask during the hiring process.\n",
    "\n",
    "        \"\"\",\n",
    "    )\n",
    "    model_response = llm_model(Que_template.format(data=Input_text))\n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb52bdc8-c1f7-4a47-ae3a-ff0332784a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        <p>\n",
      "            This is a great place to share your thoughts and opinions with the community!\n",
      "            If you are new to this, we recommend that you read our [guide on how to write a blog](https://researchers.withgoogle.com/blog-writing-guide).\n",
      "            \n",
      "        <p>\n",
      "            Before starting, please make sure to follow the guidelines in our [Researcher Blog Post Template](https://researchers.withgoogle.com/blog-template) and read through some of the blogs that have been published by your peers on this platform!\n",
      "            \n",
      "        <p>\n",
      "            When you're ready to publish, please submit a PR with your content below using the following template:\n",
      "        \n",
      "        <pre><code>\n",
      "          title: Researcher Blog Post Template\n",
      "          \n",
      "          description: This is a great place to share your thoughts and opinions with the community! If you are new to this, we recommend that you read our [guide on how to write a blog](https://researchers.withgoogle.com/blog-writing-guide).\n",
      "                  Before starting, please make sure to follow the guidelines in our\n"
     ]
    }
   ],
   "source": [
    "def getLLAMAresponse(input_text, no_words, blog_style):\n",
    "    # LLAMA2 model\n",
    "    llm = CTransformers(\n",
    "        model=\"../Artifacts/llama-2-7b.ggmlv3.q4_1.bin\",\n",
    "        model_type=\"llama\",\n",
    "        config={\"max_new_tokens\": 256, \"temperature\": 0.5},\n",
    "    )\n",
    "\n",
    "    # Prompt Template\n",
    "    template = \"\"\"\n",
    "        Write a blog for {blog_style} job profile for a topic {input_text}\n",
    "        within {no_words} words.\n",
    "            \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"blog_style\", \"input_text\", \"no_words\"], template=template\n",
    "    )\n",
    "\n",
    "    # Generate the response from the LLAMA2 model\n",
    "    response = llm(\n",
    "        prompt.format(blog_style=blog_style, input_text=input_text, no_words=no_words)\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "result = getLLAMAresponse(\"MLOPS\", 300, \"Researcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56ef0954-342e-42e8-98bc-b884827c9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Write a blog for Data Scientist job profile for a topic MLOPS\n",
      "        within 250 words.\n",
      "        \n",
      "        Write a blog for Data Engineer job profile for a topic MLOPS\n",
      "        within 150 words.\n",
      "    </div>\n",
      "    \n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99e4e707-f733-4514-bca9-b2043c8b4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Write a blog for Data Scientist job profile for a topic MLOPS\n",
      "        within 250 words.\n",
      "        \n",
      "        Write a blog for Data Engineer job profile for a topic MLOPS\n",
      "        within 150 words.\n",
      "    </div>\n",
      "    \n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d655db-c10e-427b-b994-0524d2b0b1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa8ea2-2e7b-42ef-9d52-a0e3f24e4b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4963ccf9-28c2-4e18-a425-515949158967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of tokens (513) exceeded maximum context length (512).\n",
      "Number of tokens (514) exceeded maximum context length (512).\n",
      "Number of tokens (515) exceeded maximum context length (512).\n",
      "Number of tokens (516) exceeded maximum context length (512).\n",
      "Number of tokens (517) exceeded maximum context length (512).\n",
      "Number of tokens (518) exceeded maximum context length (512).\n",
      "Number of tokens (519) exceeded maximum context length (512).\n",
      "Number of tokens (520) exceeded maximum context length (512).\n",
      "Number of tokens (521) exceeded maximum context length (512).\n",
      "Number of tokens (522) exceeded maximum context length (512).\n",
      "Number of tokens (523) exceeded maximum context length (512).\n",
      "Number of tokens (524) exceeded maximum context length (512).\n",
      "Number of tokens (525) exceeded maximum context length (512).\n",
      "Number of tokens (526) exceeded maximum context length (512).\n",
      "Number of tokens (527) exceeded maximum context length (512).\n",
      "Number of tokens (528) exceeded maximum context length (512).\n",
      "Number of tokens (529) exceeded maximum context length (512).\n",
      "Number of tokens (530) exceeded maximum context length (512).\n",
      "Number of tokens (531) exceeded maximum context length (512).\n",
      "Number of tokens (532) exceeded maximum context length (512).\n",
      "Number of tokens (533) exceeded maximum context length (512).\n",
      "Number of tokens (534) exceeded maximum context length (512).\n",
      "Number of tokens (535) exceeded maximum context length (512).\n",
      "Number of tokens (536) exceeded maximum context length (512).\n",
      "Number of tokens (537) exceeded maximum context length (512).\n",
      "Number of tokens (538) exceeded maximum context length (512).\n",
      "Number of tokens (539) exceeded maximum context length (512).\n",
      "Number of tokens (540) exceeded maximum context length (512).\n",
      "Number of tokens (541) exceeded maximum context length (512).\n",
      "Number of tokens (542) exceeded maximum context length (512).\n",
      "Number of tokens (543) exceeded maximum context length (512).\n",
      "Number of tokens (544) exceeded maximum context length (512).\n",
      "Number of tokens (545) exceeded maximum context length (512).\n",
      "Number of tokens (546) exceeded maximum context length (512).\n",
      "Number of tokens (547) exceeded maximum context length (512).\n",
      "Number of tokens (548) exceeded maximum context length (512).\n",
      "Number of tokens (549) exceeded maximum context length (512).\n",
      "Number of tokens (550) exceeded maximum context length (512).\n",
      "Number of tokens (551) exceeded maximum context length (512).\n",
      "Number of tokens (552) exceeded maximum context length (512).\n",
      "Number of tokens (553) exceeded maximum context length (512).\n",
      "Number of tokens (554) exceeded maximum context length (512).\n",
      "Number of tokens (555) exceeded maximum context length (512).\n",
      "Number of tokens (556) exceeded maximum context length (512).\n",
      "Number of tokens (557) exceeded maximum context length (512).\n"
     ]
    }
   ],
   "source": [
    "result = llama_responses(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba908ac-4784-4186-95b9-96a7f228c109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "375bebd4-3652-49be-b2db-7a6f18914cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(\"../Artifacts/Yuvraj_21BCS6343.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3acd088-fd25-4f71-bf74-248e65e54263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a499b5-8b04-4d9a-8bfe-a343acffb06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee761c-7967-445b-9bca-4b0ee3dc789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if pdf_text != \"\":\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a7401-e08e-4471-b4eb-0d5487bf201d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39fc45-7fc4-46f2-a680-f9257b8b36e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Tutor",
   "language": "python",
   "name": "ai_tutor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
