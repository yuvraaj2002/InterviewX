{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa94598f-4df3-4f4b-9bf1-dd2be39e3de1",
   "metadata": {},
   "source": [
    "# Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f479b0a7-48b8-446b-bf50-750fa06228c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ropar_i9941t3\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Everything imported successfully 😃\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "print(\"Everything imported successfully 😃\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723d43b-35ed-497b-9ee8-00dc2c010af7",
   "metadata": {},
   "source": [
    "# Extracting content from blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "faaabb2a-257a-4bcf-919a-a449d770ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.codecontent.net/post/introduction-to-llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c21d199a-f989-4272-b791-5e70e175db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_process(URL):\n",
    "    try:\n",
    "        # Extracting the html content from the given URL\n",
    "        response = requests.get(URL)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            # Instantiating BeautifulSoup class\n",
    "            bsoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Extracting the main article content\n",
    "            article_content = bsoup.find_all([\"h1\", \"p\"])\n",
    "\n",
    "            if article_content:\n",
    "\n",
    "                # Extracting the text from paragraphs within the article\n",
    "                text = [content.text for content in article_content]\n",
    "                ARTICLE = \" \".join(text)\n",
    "                return ARTICLE\n",
    "\n",
    "            else:\n",
    "                return \"Unable to extract article content. Please check the URL or website structure.\"\n",
    "        else:\n",
    "            return f\"Failed to retrieve content. Status Code: {response.status_code}\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cff7ccb0-ea4f-417e-86f5-36741591e808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This article explores the transformative journey of Meta\\'s LlaMA series, from the foundational LLaMA 1, through the enhanced LLaMA 2, to the cutting-edge LLaMA 3, showcasing its significant advancements in AI with a focus on increased speed, multilingual capabilities, and sophisticated machine learning technologies that push the boundaries of language model development and application. In the dynamic realm of artificial intelligence, Large Language Models (LLMs) like LLaMA, developed by Meta (formerly known as Facebook Inc.), are pivotal, driving significant advancements in technology. The term \"Meta\" refers to the tech giant that has expanded its focus from social media to broader technological innovations, including AI research. The LLaMA series, which stands for \"Large Language Model Meta AI,\" showcases this progression in machine learning and natural language processing. These models operate by predicting the next word from a sequence of words inputted, thus generating coherent and contextually relevant text. Distinguished by its open-source nature, LLaMA stands apart in an industry where many powerful models are proprietary. This openness encourages a broad base of innovation, allowing developers, researchers, and even hobbyists to experiment and improve on LLaMA\\'s capabilities without significant costs. Each iteration, from LLaMA 1 to the most advanced LLaMA 3, builds upon the previous successes, enhancing functionalities and introducing new features that more closely mimic human cognitive processes. This blog will explore the transformative journey of the LLaMA models, focusing particularly on LLaMA 3. As the pinnacle of Meta\\'s R&D efforts, LLaMA 3 not only pushes AI closer to human-like understanding and interaction but also revolutionizes how machines interpret and generate text. With its robust performance and open-source framework, LLaMA 3 expands the boundaries of what AI can achieve, reshaping how we think about machine intelligence. LLaMA 1 marked a significant breakthrough in the world of artificial intelligence by addressing complex language processing challenges. As the pioneer in its series, it was designed to enhance scalability and deepen linguistic comprehension, utilizing innovative technologies to effectively manage extensive language data. This foundational model set the stage for the subsequent advancements in the LLaMA series, establishing a robust framework for future development. Despite its groundbreaking approach, LLaMA 1 encountered several challenges. It struggled with multilingual representation and the efficient processing of large-scale datasets, highlighting the need for more versatile and potent models. These challenges underscored the necessity for a model capable of adapting to diverse linguistic demands. The experience gained from these hurdles informed the enhancements in subsequent iterations, each designed to surpass the capabilities of its predecessor and better meet the evolving demands of AI applications. Building on the solid groundwork laid by its predecessor, LLaMA 2 emerged as a transformative leap forward in the LLaMA series. This iteration was not just an incremental upgrade; it was a comprehensive overhaul that significantly boosted computational efficiency and expanded multilingual support. By refining the model\\'s architecture and enhancing its linguistic capabilities, LLaMA 2 achieved a remarkable 50% increase in processing speed and a 40% improvement in accuracy, processing information across a diverse array of languages more effectively. LLaMA 2 introduced support for over 30 languages, a substantial increase from the fewer languages covered by LLaMA 1, making it far more versatile for global applications. This enhancement greatly improved the model\\'s usability and performance, making it a favorite among tech enthusiasts and industry professionals alike. The positive reception of LLaMA 2 was pivotal. It not only validated the series\\' approach to tackling complex language processing challenges but also fueled further innovations. Feedback from the community and insights from real-world applications were instrumental in shaping the development of LLaMA 3. These interactions underscored the need for even more advanced features, setting the stage for the next evolution. LLaMA 2\\'s enhancements were critical in paving the way for LLaMA 3, which would integrate even more sophisticated capabilities to handle complex tasks and larger datasets. This seamless progression illustrates a thoughtful response to user and community feedback, ensuring that each new version of LLaMA not only meets but exceeds the expectations set by its forerunners. The release of LLaMA 3 marks a significant advancement in artificial intelligence, particularly in language model development. This version isn\\'t just a minor update but a substantial leap that incorporates advanced machine learning technologies to manage more complex tasks with exceptional efficiency. LLaMA 3 is particularly noted for improving decision-making abilities and handling demanding tasks like in-depth reasoning and sophisticated coding. Building on LLaMA 2\\'s robust foundation, LLaMA 3 employs several advanced fine-tuning strategies to enhance its functionality. Techniques like Supervised Fine-Tuning (SFT) and Rejection Sampling refine the model\\'s performance by optimizing parameters and focusing on challenging data. Furthermore, Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) significantly boost the modelâ\\x80\\x99s decision-making capabilities and alignment with human preferences. Additionally, LLaMA 3\\'s capabilities in processing multiple languages have seen substantial growth, supporting over 30 languages and doubling its context length to 8192 tokensâ\\x80\\x94significantly expanding from LLaMA 2. This upgrade, coupled with a training regimen on a dataset roughly seven times larger than its predecessor\\'s, enhances its speed and accuracy, enabling it to understand and process a broader range of linguistic nuances effectively. The anticipation and subsequent launch of LLaMA 3 highlighted its potential to dramatically enhance data processing and tackle complex linguistic tasks. Having demonstrated substantial improvements post-launch, LLaMA 3 sets new industry standards, redefining what language models can achieve and broadening their application across various fields, from customer support to complex data analysis, thereby ushering in a new era in AI capabilities. LLaMA 3 represents more than just an improvement over its predecessorsâ\\x80\\x94it is a monumental leap in the AI landscape, significantly advancing how language models are developed and utilized. This iteration is distinguished by its integration of novel AI techniques and considerable enhancements in data handling and model training, thereby setting a new benchmark for what these technologies can achieve. The essence of LLaMA 3\\'s technological advancement lies in its revolutionary model architecture. LLaMA 3 is equipped with a new tokenizer capable of handling an impressive 128,000 different tokensâ\\x80\\x94substantially increasing from the 50,000 tokens its predecessor managed. This capability allows LLaMA 3 to process information with unprecedented speed and accuracy, dramatically enhancing its efficiency across diverse languages and tasks. Specifically, LLaMA 3 has demonstrated up to a 35% increase in processing speed and a 40% improvement in the accuracy of generated content compared to LLaMA 2. Data is crucial for any AI system, and LLaMA 3 manages it with exceptional skill. Trained on a dataset approximately seven times larger than that used for LLaMA 2, encompassing over 15 trillion tokens, LLaMA 3 has access to a richer and more varied compilation of information. It incorporates texts from news articles, books, and websites across more than 30 languages, allowing it to learn from a broad spectrum of human knowledge. This extensive training ensures that LLaMA 3â\\x80\\x99s applications are as adaptable and robust as possible. Efficiency in LLaMA 3 isnâ\\x80\\x99t only about speedâ\\x80\\x94itâ\\x80\\x99s also about intelligent scaling. LLaMA 3 employs advanced parallelization strategies, such as data and model parallelism across custom-built 24,000 GPU clusters, enhancing its ability to handle massive datasets without sacrificing performance. This scalability is vital for processing the large volumes of data LLaMA 3 is trained on and for performing complex language tasks that demand substantial computational power. These technological advancements make LLaMA 3 a powerhouse in the realm of AI, capable of transforming vast amounts of data into actionable insights and sophisticated responses without the intense resource demands typically associated with such tasks. With these improvements, LLaMA 3 sets a new standard for efficiency, scalability, and performance in AI-powered language processing. LLaMA 3 has significantly raised the bar for performance metrics in language models, outstripping its predecessors and setting new industry standards. The model\\'s enhanced capabilities are reflected in various benchmarks, where it demonstrates remarkable improvements in speed, accuracy, and efficiency. â\\x80\\x8dLLaMA 3 leverages advanced hardware and software optimizations, including the use of grouped query attention (GQA) mechanisms that streamline computational processes and enhance model responsiveness. This not only improves the speed at which the model processes information but also reduces the overall computational load, allowing for quicker and more efficient data handlingâ\\x80\\x8b â\\x80\\x8b. LLaMA 3 has demonstrated remarkable performance improvements across a range of benchmarks, significantly enhancing its capabilities in language comprehension, code generation, and more. Notably, LLaMA 3 excels in the MMLU (Massive Multitask Language Understanding) and HumanEval benchmarks, which are critical for assessing AI performance in natural language understanding and code synthesis, respectively. MMLU evaluates an AI\\'s understanding across various domains by testing it with a set of complex multiple-choice questions. HumanEval, on the other hand, challenges the model to generate code snippets based on given prompts, assessing its proficiency in code generation. LLaMA 3\\'s performance in these benchmarks illustrates its enhanced ability to handle tasks that require deep reasoning, such as summarizing long texts, debugging and explaining code, among others. For example, LLaMA 3 has shown substantial gains in these areas, scoring 82 on MMLU and 81.7 on HumanEval, indicating significant improvements in its ability to process and generate content that requires high-level cognitive capabilitiesâ\\x80\\x8b. These scores reflect the model\\'s advanced training and fine-tuning, which have been rigorously designed to meet the demands of increasingly complex AI applications. The training process for LLaMA 3 involves a significantly larger and more diverse dataset than its predecessors. It includes a broad range of data sourced from publicly accessible content, encompassing multiple languages and coding data, which is meticulously filtered to ensure high quality. This extensive dataset enables the model to achieve higher accuracy and adaptability across different languages and tasksâ\\x80\\x8b . Meta has implemented rigorous scaling and optimization strategies during the training phase, utilizing custom-built GPU clusters to maximize performance and efficiency. These efforts allow LLaMA 3 to handle extensive datasets without sacrificing performance, making it a powerhouse in both training and real-time applicationsâ\\x80\\x8bÂ\\xa0 â\\x80\\x8dLLaMA 3 has proven its mettle through its application in complex natural language processing (NLP) tasks, marking it as a standout model currently trending on the AI developer hub, Hugging Face. This reflects its broad adoption and effectiveness in real-world scenarios where nuanced understanding and decision-making are crucial. LLaMA 3 excels in diverse tasks such as content generation, translating languages, summarizing extensive texts, and even more specialized applications like legal analysis and medical research. A notable example of its real-world implementation is its integration into Meta\\'s platforms, where it enhances interactive user experiences through sophisticated chatbot functionalities. This allows for real-time communication capabilities across Meta\\'s applications, making LLaMA 3 an integral part of their ecosystem. The model\\'s ability to handle these demanding tasks with high accuracy and speed is a testament to its advanced capabilities, driven by its training on a massive dataset of 15 trillion tokens and support for over 30 languages. These practical applications underscore LLaMA 3\\'s versatility and power, showcasing its impact not only as a technological innovation but also as a tool that drives industry standards forward in the deployment of AI solutions. In this section, we explore the fine-tuning of the Llama-3-8B model, chosen specifically for its compatibility with the T4 GPUs available on Google Colab. This makes it a practical option for developers without access to more advanced computing resources. We\\'ll walk through coding examples that demonstrate how to effectively adapt this model to achieve better performance on specialized tasks. The Llama-3-8B model is designed to be lighter on computational demands while still delivering robust performance across various NLP tasks, making it ideal for environments like Google Colab which provides T4 GPUs with limited VRAM. This choice allows for efficient fine-tuning without the risk of overwhelming the available hardware. Llama-3 also comes in a 70B variant, which significantly increases computational requirements, needing more robust GPU setups such as those with at least 64GB of RAM, far exceeding what\\'s available on standard cloud-based platforms like Colabâ\\x80\\x8bâ\\x80\\x8b. â\\x80\\x8dStep 1 : This script configures a Python environment in a Jupyter notebook for advanced machine learning tasks. It uses %%capture to suppress installation output, installs Unsloth from GitHub tailored for Colab, and adds essential libraries like xformers and accelerate without dependencies to avoid conflicts. â\\x80\\x8d â\\x80\\x8dStep 2: In this step we initialize the FastLanguageModel from the unsloth library, setting parameters like maximum sequence length and data type, and enabling 4-bit quantization to optimize memory use. â\\x80\\x8d Step 3: WeÂ\\xa0 then loads a pre-quantized model, specifically unsloth/llama-3-8b-bnb-4bit, which is designed for efficient downloading and reduced memory issues. â\\x80\\x8d Step 4 : This step enhances the loaded model using the get_peft_model method to implement parameter-efficient fine-tuning techniques like LoRA. It customizes the model\\'s architecture by modifying specific projection layers and optimizing for low memory use and larger batch sizes with settings like zero dropout, no bias, and gradient checkpointing named \"unsloth\" for reduced VRAM usage. â\\x80\\x8d Step 5 : Â\\xa0This step involves loading the DailyDialog dataset to understand its structure, crucial for designing subsequent data preparation steps. The structure is printed to give insights into the datasetâ\\x80\\x99s format and content, facilitating better planning for data manipulation and model training â\\x80\\x8d â\\x80\\x8dStep 6 :Â\\xa0 DailyDialog dataset is loaded specifically with the \\'train\\' split to prepare it for model training. A custom prompt structure is defined to format dialogues for response generation tasks. The dataset is then transformed using a mapping function, which formats each dialogue by extracting the last two utterances as context, preparing the data for the model to generate appropriate responses. This step optimizes the input format for better training outcomes on conversational tasks â\\x80\\x8d Step 7 : This step sets up the training configuration for a language model using the SFTTrainer. It converts formatted text data into a training dataset, ensures the tokenizer has a padding token, and initializes training parameters like batch size, learning rate, and optimization settings. This configuration is aimed at efficient model training with tailored resource management and precision settings based on hardware capabilities. â\\x80\\x8d Step 8 : This Python code initializes a training session for a language model using the SFTTrainer class. It sets up training parameters, including dataset details, maximum sequence length, processor usage, and specific training arguments like batch size, learning rate, and optimization settings, aiming to efficiently enhance the model\\'s performance. â\\x80\\x8d â\\x80\\x8dStep 9 : We start the training process for the model and store the training statistics in trainer_stats. At step 60 we get a training loss of 1.340600. â\\x80\\x8d â\\x80\\x8dStep 10 : This script prepares the model for generating responses in dialogue interactions. It sets the model for inference, constructs a custom prompt, and processes an example dialogue through tokenization and GPU acceleration. The model then generates a response, which is decoded into human-readable text and displayed alongside the original dialogueâ\\x80\\x8d The provided examples showcase the model\\'s ability to generate contextually aware and realistic responses across various conversational scenarios. In each case, the model demonstrates an understanding of social cues, practical reasoning, and the appropriate emotional tone.  Whether responding to a casual social invitation, addressing a question about the weather, or advising on project scheduling, the model effectively mirrors human conversational behavior, suggesting it can handle a diverse range of dialogue types with relevance and coherence. This versatility makes it a valuable tool for applications requiring nuanced human-like interactions. As we\\'ve explored throughout this blog, the LLaMA series, developed by Meta, represents a significant advancement in the field of artificial intelligence. From LLaMA 1\\'s foundational breakthroughs to LLaMA 2\\'s enhancements in speed and multilingual support, and finally , to the cutting-edge capabilities of LLaMA 3, each iteration has markedly pushed the boundaries of what AI can achieve. LLaMA 3, in particular, with its sophisticated machine learning technologies and expansive training on a massive dataset, exemplifies the pinnacle of this developmental journey. This series not only enhances our understanding of complex data but also broadens the scope of AI\\'s applicability across different languages and tasks, making significant strides in making AI more versatile and accessible. The open-source nature of LLaMA further democratizes AI technology, empowering a global community of developers and researchers to innovate and drive the technology forward. In summary, the LLaMA models showcase the remarkable potential of AI to mimic human cognitive abilities, promising a future where machines can think and communicate with human-like complexity and subtlety. As AI continues to evolve, the LLaMA series will undoubtedly play a crucial role in shaping this exciting frontier CodeContent is committed to empowering businesses and individuals with innovative solutions to drive success. 2810,N,Church StÂ\\xa0WilmingtonÂ\\xa0DEÂ\\xa019802-4447Â\\xa0US'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if everythinng is working\n",
    "extract_process(\"https://www.codecontent.net/post/introduction-to-llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6371d-fef3-4622-b214-e5cf0696382a",
   "metadata": {},
   "source": [
    "Now before splitting the sentnces in the ARTICLE we will first replace all the `.,?` with `<eos>` tag and doing so have several benefits which are mentioned below\n",
    "\n",
    "- Punctuation, particularly full stops, question marks, and exclamation marks, often indicate the end of a sentence. Replacing them with \"eos\" markers explicitly clarifies these boundaries, especially when dealing with ambiguously punctuated text or mixed languages.\n",
    "- This explicit marking simplifies the tokenization process, ensuring each token falls within a well-defined sentence unit. This can be crucial for tasks like sentiment analysis, question answering, or language modeling, where understanding sentence structure is important.\n",
    "- Treating punctuation as separate tokens can sometimes lead to issues in downstream tasks. Replacing them with \"eos\" allows them to be treated specially during processing. For example, in sentiment analysis, you might want to exclude punctuation when calculating sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba8ac0-586c-4b61-aac2-9789b83ab63f",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "There are several reasons why we typically use the same tokenizer and same model when working with libraries like Hugging Face Transformers:\n",
    "\n",
    "1. Compatibility: Models and their corresponding tokenizers are trained together specifically for a particular vocabulary and input format. Using a different tokenizer might lead to mismatched vocabulary tokens, numerical IDs, and ultimately incorrect data representations for the model. This can result in unexpected behavior and poor performance.\n",
    "\n",
    "2. Consistency: By using the recommended tokenizer, you ensure that the input data is tokenized according to the way the model was trained. This consistency avoids introducing unnecessary variations that could potentially affect the model's predictions.\n",
    "\n",
    "3. Pre-built vocabulary: When you use the model's tokenizer, you benefit from having the model's vocabulary readily available. This saves you the effort of building your own vocabulary and potential issues with missing words or inconsistent representations.\n",
    "\n",
    "4. Optimization: The tokenizer and model are likely optimized to work together efficiently. Using a different tokenizer might require additional processing or introduce inefficiencies in the data conversion pipeline.\n",
    "\n",
    "`T5 was mostly trained using 512 input tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21e825d4-8c81-4e9b-9e65-c769e37b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the content into 1 single text\n",
    "ARTICLE = extract_process(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c0f520b-15ae-4876-adc1-27d1ae9fb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = ARTICLE.replace(\".\", \".<eos>\")\n",
    "ARTICLE = ARTICLE.replace(\"?\", \"?<eos>\")\n",
    "ARTICLE = ARTICLE.replace(\"!\", \"!<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0728b5ee-2de4-49c8-958a-60c5ce729f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_creation(article, max_chunk=512):\n",
    "    \"\"\"\n",
    "    Chunks an article into sentences, respecting sentence boundaries and a maximum chunk size.\n",
    "\n",
    "    Args:\n",
    "        article: The text of the article to be chunked.\n",
    "        max_chunk: The maximum number of tokens allowed in a chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of chunks, each represented as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the article into sentences based on the '<eos>' marker.\n",
    "    sentences = article.split(\"<eos>\")\n",
    "\n",
    "    # Initialize variables for chunk creation.\n",
    "    current_chunk = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1:\n",
    "            if (\n",
    "                len(chunks[current_chunk].split(\" \")) + len(sentence.split(\" \"))\n",
    "                <= max_chunk\n",
    "            ):\n",
    "                chunks[current_chunk] += \" \" + sentence\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence)\n",
    "        else:\n",
    "            chunks.append(sentence)\n",
    "\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = chunks[chunk_id].strip()\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "758448e8-f2c0-4663-bac9-dd2acf1d018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS = chunk_creation(ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c1cffe7-1357-43a0-ad72-070b81079235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This article explores the transformative journey of Meta\\'s LlaMA series, from the foundational LLaMA 1, through the enhanced LLaMA 2, to the cutting-edge LLaMA 3, showcasing its significant advancements in AI with a focus on increased speed, multilingual capabilities, and sophisticated machine learning technologies that push the boundaries of language model development and application.  In the dynamic realm of artificial intelligence, Large Language Models (LLMs) like LLaMA, developed by Meta (formerly known as Facebook Inc. ), are pivotal, driving significant advancements in technology.  The term \"Meta\" refers to the tech giant that has expanded its focus from social media to broader technological innovations, including AI research.  The LLaMA series, which stands for \"Large Language Model Meta AI,\" showcases this progression in machine learning and natural language processing.  These models operate by predicting the next word from a sequence of words inputted, thus generating coherent and contextually relevant text.  Distinguished by its open-source nature, LLaMA stands apart in an industry where many powerful models are proprietary.  This openness encourages a broad base of innovation, allowing developers, researchers, and even hobbyists to experiment and improve on LLaMA\\'s capabilities without significant costs.  Each iteration, from LLaMA 1 to the most advanced LLaMA 3, builds upon the previous successes, enhancing functionalities and introducing new features that more closely mimic human cognitive processes.  This blog will explore the transformative journey of the LLaMA models, focusing particularly on LLaMA 3.  As the pinnacle of Meta\\'s R&D efforts, LLaMA 3 not only pushes AI closer to human-like understanding and interaction but also revolutionizes how machines interpret and generate text.  With its robust performance and open-source framework, LLaMA 3 expands the boundaries of what AI can achieve, reshaping how we think about machine intelligence.  LLaMA 1 marked a significant breakthrough in the world of artificial intelligence by addressing complex language processing challenges.  As the pioneer in its series, it was designed to enhance scalability and deepen linguistic comprehension, utilizing innovative technologies to effectively manage extensive language data.  This foundational model set the stage for the subsequent advancements in the LLaMA series, establishing a robust framework for future development.  Despite its groundbreaking approach, LLaMA 1 encountered several challenges.  It struggled with multilingual representation and the efficient processing of large-scale datasets, highlighting the need for more versatile and potent models.  These challenges underscored the necessity for a model capable of adapting to diverse linguistic demands.  The experience gained from these hurdles informed the enhancements in subsequent iterations, each designed to surpass the capabilities of its predecessor and better meet the evolving demands of AI applications.  Building on the solid groundwork laid by its predecessor, LLaMA 2 emerged as a transformative leap forward in the LLaMA series.  This iteration was not just an incremental upgrade; it was a comprehensive overhaul that significantly boosted computational efficiency and expanded multilingual support.',\n",
       " \"By refining the model's architecture and enhancing its linguistic capabilities, LLaMA 2 achieved a remarkable 50% increase in processing speed and a 40% improvement in accuracy, processing information across a diverse array of languages more effectively.  LLaMA 2 introduced support for over 30 languages, a substantial increase from the fewer languages covered by LLaMA 1, making it far more versatile for global applications.  This enhancement greatly improved the model's usability and performance, making it a favorite among tech enthusiasts and industry professionals alike.  The positive reception of LLaMA 2 was pivotal.  It not only validated the series' approach to tackling complex language processing challenges but also fueled further innovations.  Feedback from the community and insights from real-world applications were instrumental in shaping the development of LLaMA 3.  These interactions underscored the need for even more advanced features, setting the stage for the next evolution.  LLaMA 2's enhancements were critical in paving the way for LLaMA 3, which would integrate even more sophisticated capabilities to handle complex tasks and larger datasets.  This seamless progression illustrates a thoughtful response to user and community feedback, ensuring that each new version of LLaMA not only meets but exceeds the expectations set by its forerunners.  The release of LLaMA 3 marks a significant advancement in artificial intelligence, particularly in language model development.  This version isn't just a minor update but a substantial leap that incorporates advanced machine learning technologies to manage more complex tasks with exceptional efficiency.  LLaMA 3 is particularly noted for improving decision-making abilities and handling demanding tasks like in-depth reasoning and sophisticated coding.  Building on LLaMA 2's robust foundation, LLaMA 3 employs several advanced fine-tuning strategies to enhance its functionality.  Techniques like Supervised Fine-Tuning (SFT) and Rejection Sampling refine the model's performance by optimizing parameters and focusing on challenging data.  Furthermore, Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) significantly boost the modelâ\\x80\\x99s decision-making capabilities and alignment with human preferences.  Additionally, LLaMA 3's capabilities in processing multiple languages have seen substantial growth, supporting over 30 languages and doubling its context length to 8192 tokensâ\\x80\\x94significantly expanding from LLaMA 2.  This upgrade, coupled with a training regimen on a dataset roughly seven times larger than its predecessor's, enhances its speed and accuracy, enabling it to understand and process a broader range of linguistic nuances effectively.  The anticipation and subsequent launch of LLaMA 3 highlighted its potential to dramatically enhance data processing and tackle complex linguistic tasks.  Having demonstrated substantial improvements post-launch, LLaMA 3 sets new industry standards, redefining what language models can achieve and broadening their application across various fields, from customer support to complex data analysis, thereby ushering in a new era in AI capabilities.  LLaMA 3 represents more than just an improvement over its predecessorsâ\\x80\\x94it is a monumental leap in the AI landscape, significantly advancing how language models are developed and utilized.\",\n",
       " \"This iteration is distinguished by its integration of novel AI techniques and considerable enhancements in data handling and model training, thereby setting a new benchmark for what these technologies can achieve.  The essence of LLaMA 3's technological advancement lies in its revolutionary model architecture.  LLaMA 3 is equipped with a new tokenizer capable of handling an impressive 128,000 different tokensâ\\x80\\x94substantially increasing from the 50,000 tokens its predecessor managed.  This capability allows LLaMA 3 to process information with unprecedented speed and accuracy, dramatically enhancing its efficiency across diverse languages and tasks.  Specifically, LLaMA 3 has demonstrated up to a 35% increase in processing speed and a 40% improvement in the accuracy of generated content compared to LLaMA 2.  Data is crucial for any AI system, and LLaMA 3 manages it with exceptional skill.  Trained on a dataset approximately seven times larger than that used for LLaMA 2, encompassing over 15 trillion tokens, LLaMA 3 has access to a richer and more varied compilation of information.  It incorporates texts from news articles, books, and websites across more than 30 languages, allowing it to learn from a broad spectrum of human knowledge.  This extensive training ensures that LLaMA 3â\\x80\\x99s applications are as adaptable and robust as possible.  Efficiency in LLaMA 3 isnâ\\x80\\x99t only about speedâ\\x80\\x94itâ\\x80\\x99s also about intelligent scaling.  LLaMA 3 employs advanced parallelization strategies, such as data and model parallelism across custom-built 24,000 GPU clusters, enhancing its ability to handle massive datasets without sacrificing performance.  This scalability is vital for processing the large volumes of data LLaMA 3 is trained on and for performing complex language tasks that demand substantial computational power.  These technological advancements make LLaMA 3 a powerhouse in the realm of AI, capable of transforming vast amounts of data into actionable insights and sophisticated responses without the intense resource demands typically associated with such tasks.  With these improvements, LLaMA 3 sets a new standard for efficiency, scalability, and performance in AI-powered language processing.  LLaMA 3 has significantly raised the bar for performance metrics in language models, outstripping its predecessors and setting new industry standards.  The model's enhanced capabilities are reflected in various benchmarks, where it demonstrates remarkable improvements in speed, accuracy, and efficiency.  â\\x80\\x8dLLaMA 3 leverages advanced hardware and software optimizations, including the use of grouped query attention (GQA) mechanisms that streamline computational processes and enhance model responsiveness.  This not only improves the speed at which the model processes information but also reduces the overall computational load, allowing for quicker and more efficient data handlingâ\\x80\\x8b â\\x80\\x8b.  LLaMA 3 has demonstrated remarkable performance improvements across a range of benchmarks, significantly enhancing its capabilities in language comprehension, code generation, and more.  Notably, LLaMA 3 excels in the MMLU (Massive Multitask Language Understanding) and HumanEval benchmarks, which are critical for assessing AI performance in natural language understanding and code synthesis, respectively.  MMLU evaluates an AI's understanding across various domains by testing it with a set of complex multiple-choice questions.\",\n",
       " \"HumanEval, on the other hand, challenges the model to generate code snippets based on given prompts, assessing its proficiency in code generation.  LLaMA 3's performance in these benchmarks illustrates its enhanced ability to handle tasks that require deep reasoning, such as summarizing long texts, debugging and explaining code, among others.  For example, LLaMA 3 has shown substantial gains in these areas, scoring 82 on MMLU and 81. 7 on HumanEval, indicating significant improvements in its ability to process and generate content that requires high-level cognitive capabilitiesâ\\x80\\x8b.  These scores reflect the model's advanced training and fine-tuning, which have been rigorously designed to meet the demands of increasingly complex AI applications.  The training process for LLaMA 3 involves a significantly larger and more diverse dataset than its predecessors.  It includes a broad range of data sourced from publicly accessible content, encompassing multiple languages and coding data, which is meticulously filtered to ensure high quality.  This extensive dataset enables the model to achieve higher accuracy and adaptability across different languages and tasksâ\\x80\\x8b .  Meta has implemented rigorous scaling and optimization strategies during the training phase, utilizing custom-built GPU clusters to maximize performance and efficiency.  These efforts allow LLaMA 3 to handle extensive datasets without sacrificing performance, making it a powerhouse in both training and real-time applicationsâ\\x80\\x8bÂ\\xa0 â\\x80\\x8dLLaMA 3 has proven its mettle through its application in complex natural language processing (NLP) tasks, marking it as a standout model currently trending on the AI developer hub, Hugging Face.  This reflects its broad adoption and effectiveness in real-world scenarios where nuanced understanding and decision-making are crucial.  LLaMA 3 excels in diverse tasks such as content generation, translating languages, summarizing extensive texts, and even more specialized applications like legal analysis and medical research.  A notable example of its real-world implementation is its integration into Meta's platforms, where it enhances interactive user experiences through sophisticated chatbot functionalities.  This allows for real-time communication capabilities across Meta's applications, making LLaMA 3 an integral part of their ecosystem.  The model's ability to handle these demanding tasks with high accuracy and speed is a testament to its advanced capabilities, driven by its training on a massive dataset of 15 trillion tokens and support for over 30 languages.  These practical applications underscore LLaMA 3's versatility and power, showcasing its impact not only as a technological innovation but also as a tool that drives industry standards forward in the deployment of AI solutions.  In this section, we explore the fine-tuning of the Llama-3-8B model, chosen specifically for its compatibility with the T4 GPUs available on Google Colab.  This makes it a practical option for developers without access to more advanced computing resources.  We'll walk through coding examples that demonstrate how to effectively adapt this model to achieve better performance on specialized tasks.\",\n",
       " 'The Llama-3-8B model is designed to be lighter on computational demands while still delivering robust performance across various NLP tasks, making it ideal for environments like Google Colab which provides T4 GPUs with limited VRAM.  This choice allows for efficient fine-tuning without the risk of overwhelming the available hardware.  Llama-3 also comes in a 70B variant, which significantly increases computational requirements, needing more robust GPU setups such as those with at least 64GB of RAM, far exceeding what\\'s available on standard cloud-based platforms like Colabâ\\x80\\x8bâ\\x80\\x8b.  â\\x80\\x8dStep 1 : This script configures a Python environment in a Jupyter notebook for advanced machine learning tasks.  It uses %%capture to suppress installation output, installs Unsloth from GitHub tailored for Colab, and adds essential libraries like xformers and accelerate without dependencies to avoid conflicts.  â\\x80\\x8d â\\x80\\x8dStep 2: In this step we initialize the FastLanguageModel from the unsloth library, setting parameters like maximum sequence length and data type, and enabling 4-bit quantization to optimize memory use.  â\\x80\\x8d Step 3: WeÂ\\xa0 then loads a pre-quantized model, specifically unsloth/llama-3-8b-bnb-4bit, which is designed for efficient downloading and reduced memory issues.  â\\x80\\x8d Step 4 : This step enhances the loaded model using the get_peft_model method to implement parameter-efficient fine-tuning techniques like LoRA.  It customizes the model\\'s architecture by modifying specific projection layers and optimizing for low memory use and larger batch sizes with settings like zero dropout, no bias, and gradient checkpointing named \"unsloth\" for reduced VRAM usage.  â\\x80\\x8d Step 5 : Â\\xa0This step involves loading the DailyDialog dataset to understand its structure, crucial for designing subsequent data preparation steps.  The structure is printed to give insights into the datasetâ\\x80\\x99s format and content, facilitating better planning for data manipulation and model training â\\x80\\x8d â\\x80\\x8dStep 6 :Â\\xa0 DailyDialog dataset is loaded specifically with the \\'train\\' split to prepare it for model training.  A custom prompt structure is defined to format dialogues for response generation tasks.  The dataset is then transformed using a mapping function, which formats each dialogue by extracting the last two utterances as context, preparing the data for the model to generate appropriate responses.  This step optimizes the input format for better training outcomes on conversational tasks â\\x80\\x8d Step 7 : This step sets up the training configuration for a language model using the SFTTrainer.  It converts formatted text data into a training dataset, ensures the tokenizer has a padding token, and initializes training parameters like batch size, learning rate, and optimization settings.  This configuration is aimed at efficient model training with tailored resource management and precision settings based on hardware capabilities.  â\\x80\\x8d Step 8 : This Python code initializes a training session for a language model using the SFTTrainer class.  It sets up training parameters, including dataset details, maximum sequence length, processor usage, and specific training arguments like batch size, learning rate, and optimization settings, aiming to efficiently enhance the model\\'s performance.',\n",
       " \"â\\x80\\x8d â\\x80\\x8dStep 9 : We start the training process for the model and store the training statistics in trainer_stats.  At step 60 we get a training loss of 1. 340600.  â\\x80\\x8d â\\x80\\x8dStep 10 : This script prepares the model for generating responses in dialogue interactions.  It sets the model for inference, constructs a custom prompt, and processes an example dialogue through tokenization and GPU acceleration.  The model then generates a response, which is decoded into human-readable text and displayed alongside the original dialogueâ\\x80\\x8d The provided examples showcase the model's ability to generate contextually aware and realistic responses across various conversational scenarios.  In each case, the model demonstrates an understanding of social cues, practical reasoning, and the appropriate emotional tone.   Whether responding to a casual social invitation, addressing a question about the weather, or advising on project scheduling, the model effectively mirrors human conversational behavior, suggesting it can handle a diverse range of dialogue types with relevance and coherence.  This versatility makes it a valuable tool for applications requiring nuanced human-like interactions.  As we've explored throughout this blog, the LLaMA series, developed by Meta, represents a significant advancement in the field of artificial intelligence.  From LLaMA 1's foundational breakthroughs to LLaMA 2's enhancements in speed and multilingual support, and finally , to the cutting-edge capabilities of LLaMA 3, each iteration has markedly pushed the boundaries of what AI can achieve.  LLaMA 3, in particular, with its sophisticated machine learning technologies and expansive training on a massive dataset, exemplifies the pinnacle of this developmental journey.  This series not only enhances our understanding of complex data but also broadens the scope of AI's applicability across different languages and tasks, making significant strides in making AI more versatile and accessible.  The open-source nature of LLaMA further democratizes AI technology, empowering a global community of developers and researchers to innovate and drive the technology forward.  In summary, the LLaMA models showcase the remarkable potential of AI to mimic human cognitive abilities, promising a future where machines can think and communicate with human-like complexity and subtlety.  As AI continues to evolve, the LLaMA series will undoubtedly play a crucial role in shaping this exciting frontier CodeContent is committed to empowering businesses and individuals with innovative solutions to drive success.  2810,N,Church StÂ\\xa0WilmingtonÂ\\xa0DEÂ\\xa019802-4447Â\\xa0US\"]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf11fa-e8d2-41f5-b76f-35163cd335dd",
   "metadata": {},
   "source": [
    "# Model information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3cff75a-66ff-487e-a38b-e7bdb3af5a58",
   "metadata": {},
   "source": [
    "Developed by Google researchers, T5 is a large-scale transformer-based language model that has achieved state-of-the-art results on various NLP tasks, including text summarization. As the model is pre-trained on a mixture of unsupervised and supervised tasks, it has the potential to generalize well to new tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\n",
    "\n",
    "One of the most exciting applications of T5 is in text summarization. Summarizing lengthy documents while preserving the most relevant information is a challenging task, but T5 has achieved impressive results in this area. By inputting the text to be summarized with the prefix “summarize:”, T5 can generate a concise summary that captures the essence of the original document. This is useful for applications such as news articles, scientific papers, and legal documents. \n",
    "\n",
    "T5 comes in different sizes: t5-small,t5-base,t5-large,t5–3b and t5–11b. For our usecase we will be using the t5-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334da363-a172-4878-8419-8c0b20041296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c099bd3ef66b420dad82ad6a8b4b20fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ropar_i9941t3\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ropar_i9941t3\\.cache\\huggingface\\hub\\models--sshleifer--distilbart-cnn-12-6. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c358b3375ea643af8d1e7d39fd675bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c901eca971544cb8c35b73af04b3f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc259da42eba466081fe07f5dc19d196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480a84ae004c444f9152ca648afe06c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiating the summarization pipeline using t5-base model\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b73c0f23-3e9e-4cd9-866a-79a74132db59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = summarizer(CHUNKS, max_length=200, min_length=50, do_sample=False)\n",
    "summarized_tezt = \" \".join([summ[\"summary_text\"] for summ in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ccd9556-7a3d-4245-b5a4-2140d2a8f576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Large Language Models (LLMs) like LLaMA, developed by Meta (formerly known as Facebook Inc. ), are pivotal, driving significant advancements in technology . Each iteration builds upon the previous successes, enhancing functionalities and introducing new features that more closely mimic human cognitive processes . This article explores the transformative journey of Meta's LlaMA series .  LLaMA 2 achieved a remarkable 50% increase in processing speed and a 40% improvement in accuracy . The positive reception of the model was pivotal. Feedback from the community and insights from real-world applications were instrumental in shaping the development of LlaMA 3 . LLaMa 3 employs several advanced fine-tuning strategies to enhance its functionality .  LLaMA 3 has demonstrated up to a 35% increase in processing speed and a 40% improvement in the accuracy of generated content compared to LlaMA 2 . The AI system is equipped with a new tokenizer capable of handling an impressive 128,000 different tokens . It incorporates texts from news articles, books, and websites across more than 30 languages .  LLaMA 3 excels in diverse tasks such as content generation, translating languages, summarizing extensive texts, and even more specialized applications like legal analysis and medical research . The model's ability to handle these demanding tasks with high accuracy and speed is a testament to its advanced capabilities, driven by its training on a massive dataset of 15 trillion tokens and support for over 30 languages .  The Llama-3-8B model is designed to be lighter on computational demands while still delivering robust performance across various NLP tasks . This choice allows for efficient fine-tuning without the risk of overwhelming the available hardware . The model is ideal for environments like Google Colab which provides T4 GPUs with limited VRAM .  The LLaMA series, developed by Meta, represents a significant advancement in the field of artificial intelligence . The open-source nature of the model further democratizes AI technology, empowering developers and researchers to innovate and drive the technology forward . CodeContent is committed to empowering businesses and individuals with innovative solutions to drive success .\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_tezt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
