{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa94598f-4df3-4f4b-9bf1-dd2be39e3de1",
   "metadata": {},
   "source": [
    "# Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f479b0a7-48b8-446b-bf50-750fa06228c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 10:51:29.243895: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-19 10:51:29.281224: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-19 10:51:29.281277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-19 10:51:29.282475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-19 10:51:29.288581: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-19 10:51:29.289492: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-19 10:51:30.405682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything imported successfully 😃\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "print(\"Everything imported successfully 😃\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723d43b-35ed-497b-9ee8-00dc2c010af7",
   "metadata": {},
   "source": [
    "# Extracting content from blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faaabb2a-257a-4bcf-919a-a449d770ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://medium.com/marvelous-mlops/mlops-roadmap-2024-ff4216b8bc62\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21d199a-f989-4272-b791-5e70e175db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_process(URL):\n",
    "    try:\n",
    "        # Extracting the html content from the given URL\n",
    "        response = requests.get(URL)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Instantiating BeautifulSoup class\n",
    "            bsoup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extracting the main article content\n",
    "            article_content = bsoup.find_all(['h1', 'p'])\n",
    "            \n",
    "            if article_content:\n",
    "                \n",
    "                # Extracting the text from paragraphs within the article\n",
    "                text = [content.text for content in article_content]\n",
    "                ARTICLE = ' '.join(text)\n",
    "                return ARTICLE\n",
    "                \n",
    "            else:\n",
    "                return \"Unable to extract article content. Please check the URL or website structure.\"\n",
    "        else:\n",
    "            return f\"Failed to retrieve content. Status Code: {response.status_code}\"\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6371d-fef3-4622-b214-e5cf0696382a",
   "metadata": {},
   "source": [
    "Now before splitting the sentnces in the ARTICLE we will first replace all the `.,?` with `<eos>` tag and doing so have several benefits which are mentioned below\n",
    "\n",
    "- Punctuation, particularly full stops, question marks, and exclamation marks, often indicate the end of a sentence. Replacing them with \"eos\" markers explicitly clarifies these boundaries, especially when dealing with ambiguously punctuated text or mixed languages.\n",
    "- This explicit marking simplifies the tokenization process, ensuring each token falls within a well-defined sentence unit. This can be crucial for tasks like sentiment analysis, question answering, or language modeling, where understanding sentence structure is important.\n",
    "- Treating punctuation as separate tokens can sometimes lead to issues in downstream tasks. Replacing them with \"eos\" allows them to be treated specially during processing. For example, in sentiment analysis, you might want to exclude punctuation when calculating sentiment scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba8ac0-586c-4b61-aac2-9789b83ab63f",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "There are several reasons why we typically use the same tokenizer and same model when working with libraries like Hugging Face Transformers:\n",
    "\n",
    "1. Compatibility: Models and their corresponding tokenizers are trained together specifically for a particular vocabulary and input format. Using a different tokenizer might lead to mismatched vocabulary tokens, numerical IDs, and ultimately incorrect data representations for the model. This can result in unexpected behavior and poor performance.\n",
    "\n",
    "2. Consistency: By using the recommended tokenizer, you ensure that the input data is tokenized according to the way the model was trained. This consistency avoids introducing unnecessary variations that could potentially affect the model's predictions.\n",
    "\n",
    "3. Pre-built vocabulary: When you use the model's tokenizer, you benefit from having the model's vocabulary readily available. This saves you the effort of building your own vocabulary and potential issues with missing words or inconsistent representations.\n",
    "\n",
    "4. Optimization: The tokenizer and model are likely optimized to work together efficiently. Using a different tokenizer might require additional processing or introduce inefficiencies in the data conversion pipeline.\n",
    "\n",
    "`T5 was mostly trained using 512 input tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e825d4-8c81-4e9b-9e65-c769e37b0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting all the content into 1 single text\n",
    "ARTICLE = extract_process(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c0f520b-15ae-4876-adc1-27d1ae9fb7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = ARTICLE.replace('.', '.<eos>')\n",
    "ARTICLE = ARTICLE.replace('?', '?<eos>')\n",
    "ARTICLE = ARTICLE.replace('!', '!<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0728b5ee-2de4-49c8-958a-60c5ce729f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_creation(article, max_chunk=512):\n",
    "    \"\"\"\n",
    "    Chunks an article into sentences, respecting sentence boundaries and a maximum chunk size.\n",
    "\n",
    "    Args:\n",
    "        article: The text of the article to be chunked.\n",
    "        max_chunk: The maximum number of tokens allowed in a chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of chunks, each represented as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the article into sentences based on the '<eos>' marker.\n",
    "    sentences = article.split('<eos>')\n",
    "\n",
    "    # Initialize variables for chunk creation.\n",
    "    current_chunk = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1:\n",
    "            if len(chunks[current_chunk].split(' ')) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk] += ' ' + sentence\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence)\n",
    "        else:\n",
    "            chunks.append(sentence)\n",
    "\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = chunks[chunk_id].strip()\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "758448e8-f2c0-4663-bac9-dd2acf1d018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKS = chunk_creation(ARTICLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf11fa-e8d2-41f5-b76f-35163cd335dd",
   "metadata": {},
   "source": [
    "# Model information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3cff75a-66ff-487e-a38b-e7bdb3af5a58",
   "metadata": {},
   "source": [
    "Developed by Google researchers, T5 is a large-scale transformer-based language model that has achieved state-of-the-art results on various NLP tasks, including text summarization. As the model is pre-trained on a mixture of unsupervised and supervised tasks, it has the potential to generalize well to new tasks. The model is pre-trained on the Colossal Clean Crawled Corpus (C4), which was developed and released in the context of the same research paper as T5.\n",
    "\n",
    "One of the most exciting applications of T5 is in text summarization. Summarizing lengthy documents while preserving the most relevant information is a challenging task, but T5 has achieved impressive results in this area. By inputting the text to be summarized with the prefix “summarize:”, T5 can generate a concise summary that captures the essence of the original document. This is useful for applications such as news articles, scientific papers, and legal documents. \n",
    "\n",
    "T5 comes in different sizes: t5-small,t5-base,t5-large,t5–3b and t5–11b. For our usecase we will be using the t5-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "334da363-a172-4878-8419-8c0b20041296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small and revision d769bba (https://huggingface.co/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9416c634e44bdda186b0b5121588bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5387f021849f449698661eecbd17d458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 10:56:05.302611: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 65798144 exceeds 10% of free system memory.\n",
      "2024-02-19 10:56:05.332938: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 65798144 exceeds 10% of free system memory.\n",
      "2024-02-19 10:56:05.353539: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 65798144 exceeds 10% of free system memory.\n",
      "2024-02-19 10:56:05.935239: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 65798144 exceeds 10% of free system memory.\n",
      "2024-02-19 10:56:05.997695: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 65798144 exceeds 10% of free system memory.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b851372be444d87a349cc1759223019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66db048b73514726ba3f14126922a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fdc6af4d314a3d85c76a60d5d85f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instantiating the summarization pipeline using t5-base model\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b73c0f23-3e9e-4cd9-866a-79a74132db59",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = summarizer(CHUNKS, max_length=200, min_length=50, do_sample=False)\n",
    "summarized_tezt = ' '.join([summ['summary_text'] for summ in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ccd9556-7a3d-4245-b5a4-2140d2a8f576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLOps engineers work more on building a platform that is used by machine learning engineers and data scientists . the roadmap may have some updates during the year . we suggest starting learning Python by reading a proper Python book and practicing the concepts . MLOps engineers must be aware of the principles and factors that contribute to the maturity of the machine learning system . ML-specific orchestration systems keep all your model runs in the same place and help with: MLflow is probably the most popular tool for modeling and experiment tracking . a feature store with Feast part 1, part 2, part 3 You need to track what data was used to produce a model artifact . the answer to this question would be “it depends” . some data science teams rely on cloud-native solutions like AWS Sagemaker or Azure ML .'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_tezt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
